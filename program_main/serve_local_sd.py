#!/usr/bin/env python
"""
è‡ªåŠ¨æ£€æµ‹ GPU å¹¶å¯åŠ¨ Automatic1111 WebUIï¼ˆå¸¦ --apiï¼‰ã€‚
- GPU å¯ç”¨ âœ xformers / half precision / é«˜é€Ÿ
- GPU ä¸å¯ç”¨ âœ CPU-only æ¨¡å¼ (--precision full --no-half --skip-torch-cuda-test)

ç”¨æ³•:
    python serve_local_sd.py [--port 7860] [--model-path /your/model.safetensors]
"""
import argparse, shutil, subprocess, sys, os, pathlib, time
import torch  

ROOT = pathlib.Path(__file__).resolve().parent / "stable-diffusion-webui"

def detect_cuda() -> bool:
    return torch.cuda.is_available() and shutil.which("nvidia-smi") is not None

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--port", default="7860")
    ap.add_argument("--model-path", default="")
    args = ap.parse_args()

    if not ROOT.exists():
        print("æ‰¾ä¸åˆ° stable-diffusion-webui ç›®å½•ï¼Œè¯·å…ˆ git cloneã€‚")
        sys.exit(1)

    cmd = [sys.executable, "launch.py", "--api", "--listen", "--port", args.port]

    if detect_cuda():
        print("âœ… æ£€æµ‹åˆ° GPUï¼Œå¯ç”¨ CUDAã€‚ä»¥ xformers åŠç²¾åº¦å¯åŠ¨...")
        cmd += ["--xformers", "--medvram"]
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä»¥ CPU only æ¨¡å¼å¯åŠ¨ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰...")
        cmd += ["--precision", "full", "--no-half", "--skip-torch-cuda-test"]

    # é™„åŠ æ¨¡å‹è·¯å¾„
    if args.model_path:
        cmd += ["--ckpt", args.model_path]

    # å¯åŠ¨
    print("â–¶ å¯åŠ¨å‘½ä»¤ï¼š", " ".join(cmd))
    subprocess.Popen(cmd, cwd=ROOT)

    # ç®€å•å¿ƒè·³æ£€æµ‹
    host = f"http://127.0.0.1:{args.port}"
    import requests, time
    for _ in range(30):
        try:
            requests.get(f"{host}/sdapi/v1/sd-models", timeout=3)
            print(f"ğŸš€ WebUI å·²å°±ç»ª {host}")
            break
        except Exception:
            time.sleep(2)
    else:
        print("â° WebUI å¯åŠ¨è¶…æ—¶ï¼Œè¯·æ£€æŸ¥æ—¥å¿—çª—å£ã€‚")

if __name__ == "__main__":
    main()
